{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ODE Flow Block: Gradient Flow Validation\n",
        "\n",
        "This notebook validates that neural ODE blocks can replace transformer residual blocks without destroying gradient flow.\n",
        "\n",
        "**Experiment**: Train two tiny language models (6 layers, 256 dim) on WikiText-2:\n",
        "1. **Baseline**: Standard transformer\n",
        "2. **Hybrid**: Layers 3-4 replaced with ODE flow module\n",
        "\n",
        "**Success criteria**:\n",
        "- Gradients remain finite and non-vanishing\n",
        "- Loss decreases comparably to baseline\n",
        "- ODE module gradients are well-behaved\n",
        "\n",
        "Runtime: ~10-15 minutes on T4 GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install torchdiffeq datasets tiktoken -q\n",
        "print(\"Dependencies installed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchdiffeq import odeint\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if device.type == 'cuda':\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    \"\"\"Standard causal self-attention.\"\"\"\n",
        "    def __init__(self, d_model, n_heads, max_seq_len=512, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.qkv = nn.Linear(d_model, 3 * d_model)\n",
        "        self.proj = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('mask', torch.tril(torch.ones(max_seq_len, max_seq_len)))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, T, 3, self.n_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "        \n",
        "        att = (q @ k.transpose(-2, -1)) * (self.head_dim ** -0.5)\n",
        "        att = att.masked_fill(self.mask[:T, :T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.dropout(att)\n",
        "        \n",
        "        y = (att @ v).transpose(1, 2).reshape(B, T, C)\n",
        "        return self.proj(y)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"Standard transformer MLP.\"\"\"\n",
        "    def __init__(self, d_model, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(d_model, 4 * d_model)\n",
        "        self.fc2 = nn.Linear(4 * d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.dropout(self.fc2(F.gelu(self.fc1(x))))\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"Standard transformer block.\"\"\"\n",
        "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.attn = CausalSelfAttention(d_model, n_heads, dropout=dropout)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.mlp = MLP(d_model, dropout=dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ODEFunc(nn.Module):\n",
        "    \"\"\"Vector field for ODE flow: dH/dt = F(H, t, u)\"\"\"\n",
        "    def __init__(self, d_model, n_heads, control_dim=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.control_dim = control_dim\n",
        "        \n",
        "        # Time embedding\n",
        "        self.time_embed = nn.Sequential(\n",
        "            nn.Linear(1, d_model),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(d_model, d_model)\n",
        "        )\n",
        "        \n",
        "        # Control signal embedding\n",
        "        self.control_embed = nn.Linear(control_dim, d_model)\n",
        "        \n",
        "        # Core dynamics (simplified: attention + MLP)\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.attn = CausalSelfAttention(d_model, n_heads, dropout=dropout)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.mlp = MLP(d_model, dropout=dropout)\n",
        "        \n",
        "        # Scale output to keep dynamics stable\n",
        "        self.output_scale = nn.Parameter(torch.tensor(0.1))\n",
        "        \n",
        "        # Will be set before each forward pass\n",
        "        self.control = None\n",
        "        \n",
        "    def forward(self, t, x):\n",
        "        # t: scalar, x: (B, T, D)\n",
        "        B, T, D = x.shape\n",
        "        \n",
        "        # Time conditioning\n",
        "        t_emb = self.time_embed(t.view(1, 1).expand(B, 1)).unsqueeze(1)  # (B, 1, D)\n",
        "        \n",
        "        # Control conditioning\n",
        "        if self.control is not None:\n",
        "            c_emb = self.control_embed(self.control).unsqueeze(1)  # (B, 1, D)\n",
        "        else:\n",
        "            c_emb = 0\n",
        "            \n",
        "        # Condition the input\n",
        "        h = x + t_emb + c_emb\n",
        "        \n",
        "        # Compute vector field\n",
        "        dh = self.attn(self.ln1(h)) + self.mlp(self.ln2(h))\n",
        "        \n",
        "        return self.output_scale * dh\n",
        "\n",
        "\n",
        "class ODEFlowBlock(nn.Module):\n",
        "    \"\"\"Replaces k transformer blocks with continuous ODE flow.\"\"\"\n",
        "    def __init__(self, d_model, n_heads, control_dim=4, n_steps=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.func = ODEFunc(d_model, n_heads, control_dim, dropout)\n",
        "        self.n_steps = n_steps\n",
        "        self.register_buffer('integration_times', torch.linspace(0, 1, n_steps + 1))\n",
        "        \n",
        "    def forward(self, x, control=None):\n",
        "        self.func.control = control\n",
        "        # Use fixed-step solver for predictable compute\n",
        "        out = odeint(self.func, x, self.integration_times, method='euler')\n",
        "        return out[-1]  # Return final state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BaselineTransformer(nn.Module):\n",
        "    \"\"\"Standard transformer LM.\"\"\"\n",
        "    def __init__(self, vocab_size, d_model=256, n_heads=4, n_layers=6, max_seq_len=128, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_emb = nn.Embedding(max_seq_len, d_model)\n",
        "        self.blocks = nn.ModuleList([TransformerBlock(d_model, n_heads, dropout) for _ in range(n_layers)])\n",
        "        self.ln_f = nn.LayerNorm(d_model)\n",
        "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "        self.max_seq_len = max_seq_len\n",
        "        \n",
        "    def forward(self, idx):\n",
        "        B, T = idx.shape\n",
        "        tok = self.tok_emb(idx)\n",
        "        pos = self.pos_emb(torch.arange(T, device=idx.device))\n",
        "        x = tok + pos\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        x = self.ln_f(x)\n",
        "        return self.head(x)\n",
        "\n",
        "\n",
        "class HybridODETransformer(nn.Module):\n",
        "    \"\"\"Transformer with middle layers replaced by ODE flow.\"\"\"\n",
        "    def __init__(self, vocab_size, d_model=256, n_heads=4, n_layers=6, \n",
        "                 ode_start=2, ode_end=4, control_dim=4, n_steps=4,\n",
        "                 max_seq_len=128, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_emb = nn.Embedding(max_seq_len, d_model)\n",
        "        \n",
        "        # Early layers (standard)\n",
        "        self.early_blocks = nn.ModuleList([TransformerBlock(d_model, n_heads, dropout) \n",
        "                                           for _ in range(ode_start)])\n",
        "        \n",
        "        # ODE flow block (replaces layers ode_start to ode_end)\n",
        "        self.ode_block = ODEFlowBlock(d_model, n_heads, control_dim, n_steps, dropout)\n",
        "        \n",
        "        # Late layers (standard)\n",
        "        self.late_blocks = nn.ModuleList([TransformerBlock(d_model, n_heads, dropout) \n",
        "                                          for _ in range(n_layers - ode_end)])\n",
        "        \n",
        "        self.ln_f = nn.LayerNorm(d_model)\n",
        "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.control_dim = control_dim\n",
        "        \n",
        "    def forward(self, idx, control=None):\n",
        "        B, T = idx.shape\n",
        "        tok = self.tok_emb(idx)\n",
        "        pos = self.pos_emb(torch.arange(T, device=idx.device))\n",
        "        x = tok + pos\n",
        "        \n",
        "        # Early layers\n",
        "        for block in self.early_blocks:\n",
        "            x = block(x)\n",
        "            \n",
        "        # ODE flow\n",
        "        x = self.ode_block(x, control)\n",
        "        \n",
        "        # Late layers\n",
        "        for block in self.late_blocks:\n",
        "            x = block(x)\n",
        "            \n",
        "        x = self.ln_f(x)\n",
        "        return self.head(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import tiktoken\n",
        "\n",
        "# Load WikiText-2\n",
        "print(\"Loading WikiText-2...\")\n",
        "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n",
        "\n",
        "# Tokenize with GPT-2 tokenizer\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "text = '\\n'.join([x['text'] for x in dataset if x['text'].strip()])\n",
        "tokens = enc.encode(text)\n",
        "tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "print(f\"Total tokens: {len(tokens):,}\")\n",
        "\n",
        "vocab_size = enc.n_vocab\n",
        "print(f\"Vocab size: {vocab_size:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_batch(tokens, batch_size, seq_len, device):\n",
        "    \"\"\"Get a random batch of sequences.\"\"\"\n",
        "    ix = torch.randint(len(tokens) - seq_len - 1, (batch_size,))\n",
        "    x = torch.stack([tokens[i:i+seq_len] for i in ix]).to(device)\n",
        "    y = torch.stack([tokens[i+1:i+seq_len+1] for i in ix]).to(device)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_gradient_stats(model):\n",
        "    \"\"\"Compute gradient statistics for monitoring.\"\"\"\n",
        "    stats = {}\n",
        "    total_norm = 0.0\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.grad is not None:\n",
        "            param_norm = param.grad.data.norm(2).item()\n",
        "            total_norm += param_norm ** 2\n",
        "            # Track specific components\n",
        "            if 'ode' in name.lower():\n",
        "                stats.setdefault('ode_grad_norm', 0.0)\n",
        "                stats['ode_grad_norm'] += param_norm ** 2\n",
        "    stats['total_grad_norm'] = total_norm ** 0.5\n",
        "    if 'ode_grad_norm' in stats:\n",
        "        stats['ode_grad_norm'] = stats['ode_grad_norm'] ** 0.5\n",
        "    return stats\n",
        "\n",
        "\n",
        "def train_model(model, tokens, n_steps=500, batch_size=32, seq_len=64, lr=3e-4, log_interval=50):\n",
        "    \"\"\"Train model and return metrics.\"\"\"\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    \n",
        "    metrics = defaultdict(list)\n",
        "    \n",
        "    model.train()\n",
        "    for step in range(n_steps):\n",
        "        x, y = get_batch(tokens, batch_size, seq_len, device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x)\n",
        "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "        loss.backward()\n",
        "        \n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        \n",
        "        # Compute gradient stats before optimizer step\n",
        "        grad_stats = compute_gradient_stats(model)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        # Log metrics\n",
        "        metrics['loss'].append(loss.item())\n",
        "        metrics['grad_norm'].append(grad_stats['total_grad_norm'])\n",
        "        if 'ode_grad_norm' in grad_stats:\n",
        "            metrics['ode_grad_norm'].append(grad_stats['ode_grad_norm'])\n",
        "        \n",
        "        if step % log_interval == 0:\n",
        "            ode_info = f\", ODE grad: {grad_stats.get('ode_grad_norm', 0):.4f}\" if 'ode_grad_norm' in grad_stats else \"\"\n",
        "            print(f\"Step {step:4d} | Loss: {loss.item():.4f} | Grad norm: {grad_stats['total_grad_norm']:.4f}{ode_info}\")\n",
        "    \n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "config = {\n",
        "    'vocab_size': vocab_size,\n",
        "    'd_model': 256,\n",
        "    'n_heads': 4,\n",
        "    'n_layers': 6,\n",
        "    'max_seq_len': 128,\n",
        "    'dropout': 0.1\n",
        "}\n",
        "\n",
        "train_config = {\n",
        "    'n_steps': 500,\n",
        "    'batch_size': 32,\n",
        "    'seq_len': 64,\n",
        "    'lr': 3e-4\n",
        "}\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"EXPERIMENT: Gradient Flow Validation\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Model config: {config['n_layers']} layers, d={config['d_model']}, heads={config['n_heads']}\")\n",
        "print(f\"Training: {train_config['n_steps']} steps, batch={train_config['batch_size']}, seq_len={train_config['seq_len']}\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train baseline\n",
        "print(\"=\"*60)\n",
        "print(\"Training BASELINE transformer...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "baseline = BaselineTransformer(**config)\n",
        "n_params_baseline = sum(p.numel() for p in baseline.parameters())\n",
        "print(f\"Parameters: {n_params_baseline:,}\")\n",
        "print()\n",
        "\n",
        "baseline_metrics = train_model(baseline, tokens, **train_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train hybrid ODE model\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training HYBRID ODE transformer...\")\n",
        "print(\"(Layers 2-4 replaced with ODE flow, 4 Euler steps)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "hybrid = HybridODETransformer(\n",
        "    **config,\n",
        "    ode_start=2,  # Replace layers 2-4\n",
        "    ode_end=4,\n",
        "    control_dim=4,\n",
        "    n_steps=4  # 4 Euler steps\n",
        ")\n",
        "n_params_hybrid = sum(p.numel() for p in hybrid.parameters())\n",
        "print(f\"Parameters: {n_params_hybrid:,}\")\n",
        "print()\n",
        "\n",
        "hybrid_metrics = train_model(hybrid, tokens, **train_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# Smooth curves for visualization\n",
        "def smooth(x, window=20):\n",
        "    return np.convolve(x, np.ones(window)/window, mode='valid')\n",
        "\n",
        "# Loss curves\n",
        "ax = axes[0]\n",
        "ax.plot(smooth(baseline_metrics['loss']), label='Baseline', alpha=0.8)\n",
        "ax.plot(smooth(hybrid_metrics['loss']), label='Hybrid ODE', alpha=0.8)\n",
        "ax.set_xlabel('Step')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.set_title('Training Loss')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Gradient norms\n",
        "ax = axes[1]\n",
        "ax.plot(smooth(baseline_metrics['grad_norm']), label='Baseline', alpha=0.8)\n",
        "ax.plot(smooth(hybrid_metrics['grad_norm']), label='Hybrid ODE', alpha=0.8)\n",
        "ax.set_xlabel('Step')\n",
        "ax.set_ylabel('Gradient Norm')\n",
        "ax.set_title('Total Gradient Norm')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# ODE-specific gradients\n",
        "ax = axes[2]\n",
        "if hybrid_metrics['ode_grad_norm']:\n",
        "    ax.plot(smooth(hybrid_metrics['ode_grad_norm']), color='C1', alpha=0.8)\n",
        "    ax.set_xlabel('Step')\n",
        "    ax.set_ylabel('ODE Block Gradient Norm')\n",
        "    ax.set_title('ODE Block Gradient Flow')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add annotation about gradient health\n",
        "    mean_grad = np.mean(hybrid_metrics['ode_grad_norm'])\n",
        "    std_grad = np.std(hybrid_metrics['ode_grad_norm'])\n",
        "    ax.axhline(mean_grad, color='red', linestyle='--', alpha=0.5, label=f'Mean: {mean_grad:.3f}')\n",
        "    ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('gradient_flow_results.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nResults saved to 'gradient_flow_results.png'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nParameter counts:\")\n",
        "print(f\"  Baseline: {n_params_baseline:,}\")\n",
        "print(f\"  Hybrid:   {n_params_hybrid:,} ({100*n_params_hybrid/n_params_baseline:.1f}% of baseline)\")\n",
        "\n",
        "print(f\"\\nFinal loss (last 50 steps avg):\")\n",
        "print(f\"  Baseline: {np.mean(baseline_metrics['loss'][-50:]):.4f}\")\n",
        "print(f\"  Hybrid:   {np.mean(hybrid_metrics['loss'][-50:]):.4f}\")\n",
        "\n",
        "print(f\"\\nGradient statistics:\")\n",
        "print(f\"  Baseline grad norm (mean ± std): {np.mean(baseline_metrics['grad_norm']):.4f} ± {np.std(baseline_metrics['grad_norm']):.4f}\")\n",
        "print(f\"  Hybrid grad norm (mean ± std):   {np.mean(hybrid_metrics['grad_norm']):.4f} ± {np.std(hybrid_metrics['grad_norm']):.4f}\")\n",
        "\n",
        "if hybrid_metrics['ode_grad_norm']:\n",
        "    print(f\"  ODE block grad norm (mean ± std): {np.mean(hybrid_metrics['ode_grad_norm']):.4f} ± {np.std(hybrid_metrics['ode_grad_norm']):.4f}\")\n",
        "    \n",
        "    # Check for vanishing/exploding gradients\n",
        "    ode_grads = np.array(hybrid_metrics['ode_grad_norm'])\n",
        "    vanishing = np.sum(ode_grads < 1e-6)\n",
        "    exploding = np.sum(ode_grads > 100)\n",
        "    print(f\"\\n  Vanishing gradient steps (<1e-6): {vanishing}\")\n",
        "    print(f\"  Exploding gradient steps (>100):  {exploding}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CONCLUSION\")\n",
        "print(\"=\"*60)\n",
        "loss_ratio = np.mean(hybrid_metrics['loss'][-50:]) / np.mean(baseline_metrics['loss'][-50:])\n",
        "if loss_ratio < 1.1 and np.mean(hybrid_metrics['ode_grad_norm']) > 0.01:\n",
        "    print(\"✓ ODE flow block maintains healthy gradient flow\")\n",
        "    print(\"✓ Training loss comparable to baseline\")\n",
        "    print(\"✓ No evidence of vanishing/exploding gradients in ODE block\")\n",
        "else:\n",
        "    print(\"⚠ Results require further investigation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bonus: Control Signal Effect (Quick Test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick test: does the control signal change outputs?\n",
        "hybrid.eval()\n",
        "\n",
        "# Get a test sequence\n",
        "test_x, _ = get_batch(tokens, 1, 32, device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    # No control\n",
        "    logits_base = hybrid(test_x, control=None)\n",
        "    \n",
        "    # With control signal\n",
        "    control = torch.randn(1, 4, device=device)\n",
        "    logits_ctrl = hybrid(test_x, control=control)\n",
        "    \n",
        "    # Measure difference\n",
        "    diff = (logits_ctrl - logits_base).abs().mean().item()\n",
        "    print(f\"Mean absolute logit difference with control signal: {diff:.4f}\")\n",
        "    print(f\"(Nonzero difference confirms control signal affects outputs)\")"
      ]
    }
  ]
}
