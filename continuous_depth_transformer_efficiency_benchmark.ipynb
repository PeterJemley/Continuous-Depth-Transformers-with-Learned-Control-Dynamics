{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPMXHEX9wpyO6lnSruInZl+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JoChASvmtrkf","executionInfo":{"status":"ok","timestamp":1768431743320,"user_tz":480,"elapsed":118232,"user":{"displayName":"Peter Jemley MSHI","userId":"03334320751030864830"}},"outputId":"7556b05b-432a-445f-bd82-913548699819"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchdiffeq in /usr/local/lib/python3.12/dist-packages (0.2.5)\n","Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from torchdiffeq) (2.9.0+cpu)\n","Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from torchdiffeq) (1.16.3)\n","Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from scipy>=1.4.0->torchdiffeq) (2.0.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (3.20.2)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (3.6.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (2025.3.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.5.0->torchdiffeq) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.5.0->torchdiffeq) (3.0.3)\n","Benchmarking on: cpu\n","Instantiating models...\n","\n","--- BENCHMARK RESULTS (Batch Size: 32) ---\n","Warming up Baseline (6 Layers)...\n","-> Baseline (6 Layers) Average Time: 519.83 ms/batch\n","Warming up Hybrid ODE (4 Euler Steps)...\n","-> Hybrid ODE (4 Euler Steps) Average Time: 507.19 ms/batch\n","\n","--- EFFICIENCY SUMMARY ---\n","Baseline Time:   519.83 ms\n","Hybrid Time:     507.19 ms\n","Relative Speed:  0.98x (Hybrid is slower)\n","Actual Overhead: -2.4%\n","PASS: Overhead (0.98x) is consistent with paper claim (~1.27x).\n"]}],"source":["# %% [markdown]\n","# # Continuous-Depth Transformers: Efficiency Benchmark\n","# ## Experiment 4: Inference Latency Verification\n","#\n","# This notebook measures the runtime overhead of the Hybrid ODE architecture compared to a standard discrete transformer.\n","#\n","# **Goal:** Verify the claim that the Hybrid model incurs only ~1.27x latency overhead despite performing 4 integration steps (replacing 2 discrete layers).\n","\n","# %% [code]\n","# 1. SETUP & IMPORTS\n","!pip install torchdiffeq\n","import sys\n","import math\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchdiffeq import odeint # Changed from odeint_adjoint as odeint\n","\n","# Force deterministic behavior for fair benchmarking\n","torch.manual_seed(42)\n","\n","# Configuration matching the paper\n","CONFIG = {\n","    'd_model': 256,\n","    'n_heads': 4,\n","    'n_layers': 6,          # Baseline total layers\n","    'ode_layer_start': 2,   # Hybrid replaces layers 2 & 3\n","    'ode_layer_end': 4,\n","    'vocab_size': 33278,\n","    'seq_len': 32,\n","    'control_dim': 4,\n","    'batch_size': 32,       # Batch size for benchmarking\n","    'steps_for_ode': 4      # Fixed Euler steps for training/inference\n","}\n","\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Benchmarking on: {DEVICE}\")\n","\n","# %% [markdown]\n","# ## 2. Architecture Definitions\n","# We define both the `StandardBlock` (used in Baseline) and the `ContinuousDepthBlock` (used in Hybrid).\n","\n","# %% [code]\n","class CausalSelfAttention(nn.Module):\n","    def __init__(self, d_model, n_heads, max_len=512):\n","        super().__init__()\n","        self.c_attn = nn.Linear(d_model, 3 * d_model)\n","        self.c_proj = nn.Linear(d_model, d_model)\n","        self.n_heads = n_heads\n","        self.d_model = d_model\n","        self.register_buffer(\"bias\", torch.tril(torch.ones(max_len, max_len))\n","                                     .view(1, 1, max_len, max_len))\n","\n","    def forward(self, x):\n","        B, T, C = x.size()\n","        q, k, v = self.c_attn(x).split(self.d_model, dim=2)\n","        k = k.view(B, T, self.n_heads, C // self.n_heads).transpose(1, 2)\n","        q = q.view(B, T, self.n_heads, C // self.n_heads).transpose(1, 2)\n","        v = v.view(B, T, self.n_heads, C // self.n_heads).transpose(1, 2)\n","\n","        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n","        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n","        att = F.softmax(att, dim=-1)\n","        y = att @ v\n","        y = y.transpose(1, 2).contiguous().view(B, T, C)\n","        return self.c_proj(y)\n","\n","class MLP(nn.Module):\n","    def __init__(self, d_model):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(d_model, 4 * d_model),\n","            nn.GELU(),\n","            nn.Linear(4 * d_model, d_model)\n","        )\n","    def forward(self, x): return self.net(x)\n","\n","class StandardBlock(nn.Module):\n","    def __init__(self, d_model, n_heads):\n","        super().__init__()\n","        self.ln1 = nn.LayerNorm(d_model)\n","        self.attn = CausalSelfAttention(d_model, n_heads)\n","        self.ln2 = nn.LayerNorm(d_model)\n","        self.mlp = MLP(d_model)\n","\n","    def forward(self, x):\n","        x = x + self.attn(self.ln1(x))\n","        x = x + self.mlp(self.ln2(x))\n","        return x\n","\n","# --- ODE COMPONENTS (Explicit Injection) ---\n","class ControlledVectorField(nn.Module):\n","    \"\"\"F_theta(h, u) with explicit concatenation.\"\"\"\n","    def __init__(self, d_model, control_dim):\n","        super().__init__()\n","        input_dim = d_model + control_dim\n","        self.net = nn.Sequential(\n","            nn.Linear(input_dim, d_model * 2),\n","            nn.Softplus(),\n","            nn.Linear(d_model * 2, d_model)\n","        )\n","        # Stability initialization\n","        nn.init.normal_(self.net[-1].weight, mean=0, std=0.01)\n","        nn.init.constant_(self.net[-1].bias, 0)\n","\n","    def forward(self, t, h, u):\n","        seq_len = h.shape[1]\n","        u_expanded = u.unsqueeze(1).expand(-1, seq_len, -1)\n","        state = torch.cat([h, u_expanded], dim=-1)\n","        return self.net(state)\n","\n","class ContinuousDepthBlock(nn.Module):\n","    def __init__(self, d_model, control_dim, steps=4):\n","        super().__init__()\n","        self.vector_field = ControlledVectorField(d_model, control_dim)\n","        self.alpha = nn.Parameter(torch.tensor(0.1))\n","        self.options = {'step_size': 1.0 / steps} # Fixed step size\n","\n","    def forward(self, h, u):\n","        def func(t, x): return self.alpha * self.vector_field(t, x, u)\n","        integration_times = torch.tensor([0, 1]).float().to(h.device)\n","\n","        # Fixed Euler for efficiency benchmark (as per training)\n","        state = odeint(func, h, integration_times, method='euler', options=self.options)\n","        return state[1]\n","\n","# %% [markdown]\n","# ## 3. Model Wrappers: Baseline vs. Hybrid\n","\n","# %% [code]\n","class BaselineTransformer(nn.Module):\n","    \"\"\"Standard 6-layer Transformer (Discrete).\"\"\"\n","    def __init__(self, config):\n","        super().__init__()\n","        self.embed = nn.Embedding(config['vocab_size'], config['d_model'])\n","        self.pos_embed = nn.Parameter(torch.zeros(1, config['seq_len'], config['d_model']))\n","\n","        # 6 Standard Blocks\n","        self.layers = nn.ModuleList([\n","            StandardBlock(config['d_model'], config['n_heads'])\n","            for _ in range(config['n_layers'])\n","        ])\n","\n","        self.ln_f = nn.LayerNorm(config['d_model'])\n","        self.head = nn.Linear(config['d_model'], config['vocab_size'], bias=False)\n","\n","    def forward(self, idx, u=None): # u is ignored in baseline\n","        B, T = idx.size()\n","        x = self.embed(idx) + self.pos_embed[:, :T, :]\n","        for layer in self.layers:\n","            x = layer(x)\n","        logits = self.head(self.ln_f(x))\n","        return logits\n","\n","class HybridTransformer(nn.Module):\n","    \"\"\"Hybrid Architecture: Layers 2-3 replaced by ODE.\"\"\"\n","    def __init__(self, config):\n","        super().__init__()\n","        self.embed = nn.Embedding(config['vocab_size'], config['d_model'])\n","        self.pos_embed = nn.Parameter(torch.zeros(1, config['seq_len'], config['d_model']))\n","\n","        # Layers 0-1\n","        self.early_layers = nn.ModuleList([\n","            StandardBlock(config['d_model'], config['n_heads'])\n","            for _ in range(config['ode_layer_start'])\n","        ])\n","\n","        # Continuous Block (Replaces 2-3)\n","        self.ode_block = ContinuousDepthBlock(config['d_model'], config['control_dim'],\n","                                             steps=config['steps_for_ode'])\n","\n","        # Layers 4-5\n","        remaining = config['n_layers'] - config['ode_layer_end']\n","        self.late_layers = nn.ModuleList([\n","            StandardBlock(config['d_model'], config['n_heads'])\n","            for _ in range(remaining)\n","        ])\n","\n","        self.ln_f = nn.LayerNorm(config['d_model'])\n","        self.head = nn.Linear(config['d_model'], config['vocab_size'], bias=False)\n","\n","    def forward(self, idx, u):\n","        B, T = idx.size()\n","        x = self.embed(idx) + self.pos_embed[:, :T, :]\n","        for layer in self.early_layers: x = layer(x)\n","        x = self.ode_block(x, u)\n","        for layer in self.late_layers: x = layer(x)\n","        return self.head(self.ln_f(x))\n","\n","# %% [markdown]\n","# ## 4. The Benchmark Loop\n","\n","# %% [code]\n","def run_benchmark(model_name, model, x, u, n_warmup=10, n_runs=100):\n","    model.eval()\n","\n","    # Warmup\n","    print(f\"Warming up {model_name}...\")\n","    with torch.no_grad():\n","        for _ in range(n_warmup):\n","            _ = model(x, u)\n","\n","    # Timing\n","    torch.cuda.synchronize() if torch.cuda.is_available() else None\n","    start_time = time.perf_counter()\n","\n","    with torch.no_grad():\n","        for _ in range(n_runs):\n","            _ = model(x, u)\n","\n","    torch.cuda.synchronize() if torch.cuda.is_available() else None\n","    end_time = time.perf_counter()\n","\n","    avg_time_ms = ((end_time - start_time) / n_runs) * 1000\n","    print(f\"-> {model_name} Average Time: {avg_time_ms:.2f} ms/batch\")\n","    return avg_time_ms\n","\n","# Data Generation\n","def get_dummy_batch(config):\n","    idx = torch.randint(0, config['vocab_size'], (config['batch_size'], config['seq_len'])).to(DEVICE)\n","    u = torch.randn(config['batch_size'], config['control_dim']).to(DEVICE)\n","    return idx, u\n","\n","# Instantiate Models\n","print(\"Instantiating models...\")\n","baseline = BaselineTransformer(CONFIG).to(DEVICE)\n","hybrid = HybridTransformer(CONFIG).to(DEVICE)\n","\n","x_bench, u_bench = get_dummy_batch(CONFIG)\n","\n","print(f\"\\n--- BENCHMARK RESULTS (Batch Size: {CONFIG['batch_size']}) ---\")\n","\n","# Run Baseline\n","t_baseline = run_benchmark(\"Baseline (6 Layers)\", baseline, x_bench, u_bench)\n","\n","# Run Hybrid\n","t_hybrid = run_benchmark(\"Hybrid ODE (4 Euler Steps)\", hybrid, x_bench, u_bench)\n","\n","# Comparison\n","ratio = t_hybrid / t_baseline\n","overhead = (ratio - 1.0) * 100\n","\n","print(\"\\n--- EFFICIENCY SUMMARY ---\")\n","print(f\"Baseline Time:   {t_baseline:.2f} ms\")\n","print(f\"Hybrid Time:     {t_hybrid:.2f} ms\")\n","print(f\"Relative Speed:  {ratio:.2f}x (Hybrid is slower)\")\n","print(f\"Actual Overhead: {overhead:.1f}%\")\n","\n","target_ratio = 1.27\n","if ratio <= target_ratio + 0.05: # 5% tolerance\n","    print(f\"PASS: Overhead ({ratio:.2f}x) is consistent with paper claim (~{target_ratio}x).\")\n","else:\n","    print(f\"WARNING: Overhead ({ratio:.2f}x) exceeds paper claim (~{target_ratio}x).\")"]}]}