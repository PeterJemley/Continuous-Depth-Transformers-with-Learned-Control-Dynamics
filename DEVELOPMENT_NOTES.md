# Development Notes: A Falsificationist Approach to Continuous-Depth Transformers

This document outlines the **problem situations**, **tentative theories**, and **critical tests** that shaped the final paper. It moves away from inductive assertion ("this works because we say so") toward a deductive approach: proposing a model and attempting to break it.

---

## 1. The Problem Situation and The Conjecture

### The Shift from Inductivism to Critical Rationalism

**Original Flaw:** The initial proposal suffered from verificationism. It asserted that "language is nonlinear → neural ODEs are appropriate" without specifying conditions under which this would be false. It lacked **potential falsifiers**—concrete failure modes that would refute the theory.

**The Popperian Correction (Key Revisions):**
We redefined the project around a **bold conjecture**: *Control dynamics can be learned end-to-end within the training process itself, rather than discovered post-hoc.*

* **Demarcation:** We distinguished our theory from existing work (Neural ODEs, Activation Steering) by identifying a specific new capacity: the ability to learn a control manifold during training.
* **Risky Prediction:** We predicted that a hybrid model could achieve control *without* the massive compute overhead usually associated with ODEs. This was a risky prediction because standard ODE theory suggests a high computational cost.

---

## 2. Severe Testing (Experimental Validation)

We subjected the "Continuous-Depth" conjecture to four **crucial experiments**. A crucial experiment is one designed to yield a result that is inconsistent with the theory if the theory is false.

### Test 1: Stability and Efficiency (Attempting to Falsify Viability)

**The Null Hypothesis:** Integrating an ODE solver into a transformer will lead to vanishing gradients or exploding computational costs due to the recursive nature of the solver.

**The Test:** We replaced layers 2-3 with an ODE flow (4 Euler steps) and trained on WikiText-2.

**Corroboration:**
The theory survived the test. Not only did gradients remain stable (Norm: 0.509 vs Baseline 0.521), but the model also achieved *better* loss (6.449 vs 6.471) with *fewer* parameters.

**Additional Finding:** The learned scalar α converged from 0.1 to 0.065—a 35% reduction. This indicates the model preferred a conservative, damped update regime rather than learning to "speed up" the dynamics.

* *Note:* Popper would argue this result is highly corroborative because it was "improbable" given the background knowledge of ODE computational heaviness.

### Test 2: The "Hybrid Unfreeze" (Isolating the Mechanism)

**The Risk:** The model might appear to steer effectively by "cheating"—learning to associate the control vector u with specific output tokens directly, bypassing the continuous dynamics. This would be an *ad hoc* adjustment.

**The Severe Test:** We froze the embeddings and output head, forcing the model to learn steering *solely* through the ODE block vector field. If the ODE block could not carry the semantic load, the experiment would fail.

**Result:** The model successfully steered sentiment (98% Positive vs 88% Negative accuracy). The theory that "dynamics alone can carry semantics" was corroborated.

### Test 3: The Manifold Hypothesis (Testing for Continuity)

**The Conjecture:** The control signal u operates on a continuous manifold, not as a binary switch.

**Potential Falsifier:** If sweeping u produced a step-function (abrupt jump from "Good" to "Bad"), the "continuous depth" theory would be falsified in favor of a discrete switching model.

**Result:** The sweep from -2 to +2 produced smooth sigmoid curves. The theory of a continuous manifold stands.

### Test 4: The Solver Invariance Test (A Methodological Contribution)

**The Conjecture:** The model has learned an intrinsic continuous vector field, not merely overfitted to the fixed Euler discretization used during training.

**Potential Falsifier:** If the trajectory generated by a high-precision adaptive solver (Dopri5) diverged significantly from the fixed Euler solver, the model would be exposed as a "ResNet in disguise"—dependent on specific discretization artifacts.

**Result:** Trajectory divergence of **0.068%**. The model demonstrably learned true continuous dynamics.

**Methodological Contribution:** We propose this test as a standardized diagnostic for any continuous-depth architecture. A model claiming "continuous depth" should pass this falsification test.

---

## 3. The Logic of the Model (Explanations)

### The Governing Equation as a Universal Law

**LaTeX:** `dH/dτ = F_θ(H, τ, u)`

**Plain text:** `dH/dtau = F_theta(H, tau, u)`

Popper emphasized that good theories have high **empirical content**—they forbid many things. This equation forbids "teleportation" in the state space. It asserts that the hidden state H must evolve continuously through depth τ (tau).

**Visualizing the Conjecture:**
* **The Vector Field:** We conjecture that the model learns a vector field. "Integration" is simply following the arrows.
* **The Control Signal (u):** This is not a discrete injection but a modification of the law of motion itself. It warps the entire field, changing the trajectory of *all* points.

---

## 4. Objective Knowledge from Subjective Tools (Solver Probing)

Popper argued that "our knowledge grows through the correction of our mistakes." The use of adaptive solvers provided an unintended instrument to probe the objective structure of the learned representation.

### The Unexpected Discovery: NFE Asymmetry

We treated the solver's Number of Function Evaluations (NFE) as a measurement instrument.

* **Observation:** The solver required significantly more steps (NFE=20) to resolve "Negative" sentiment trajectories than "Positive" ones (NFE=14).
* **Interpretation:** This suggests the "Negative" region of the vector field has higher curvature or complexity.

**Why this is Popperian:**
We did not set out to prove this. The "solver as instrument" revealed a geometric reality (an asymmetry) that our initial theory did not predict but must now account for. It generated a *new problem situation*: Why is negativity geometrically more complex than positivity in WikiText-2?

**Intersubjective Testing:**
To ensure this wasn't an artifact of the instrument (the specific solver), we cross-checked with `dopri5` and `adaptive_heun`. Their agreement (within 15%) validates the objectivity of the finding.

### The "Dynamics of Resistance" Explanation

The NFE asymmetry is the geometric manifestation of fighting intrinsic bias:

* **The Natural Prior:** Under neutral conditions (u=0), the model assigns 91.4% probability to "Good" and only 3.3% to "Bad."
* **Cooperative Dynamics (u > 0):** Positive steering works *with* the natural flow—coasting into the existing basin of attraction. Low curvature, low NFE.
* **Adversarial Dynamics (u < 0):** Negative steering works *against* the natural flow—the vector field must exhibit greater curvature to forcefully divert trajectories out of the positive basin. High curvature, high NFE.

---

## 5. A Falsified Hypothesis: The "Decision Point"

### The Initial Conjecture

We initially hypothesized that the peak solver effort at τ ≈ 0.67 (tau ≈ 0.67) represented a semantic "decision point"—a moment where the model "commits" to positive or negative sentiment, analogous to a human pondering a question and then deciding.

### The Severe Test

We trained linear probes on hidden states at intermediate depths (τ = 0, 0.5, 0.67, 1.0) to measure sentiment separability. If a decision point existed, separability should spike at τ ≈ 0.67.

### The Result: Falsification

**Finding:** Accuracy was flat across all depths. No spike in separability at τ ≈ 0.67.

**Conclusion:** The "decision point" hypothesis is falsified.

### The Replacement Theory: Global Bias

The control signal u does not trigger a decision late in the process. Instead, it **tilts the entire landscape from τ = 0**.

**The Gravity Analogy:**
Imagine a ball rolling across a table with a hole in the center ("Positive," the default).

* **Positive Control (u=+1):** You tilt the table toward the hole. The ball rolls smoothly in.
* **Negative Control (u=-1):** You tilt the table away from the hole toward a corner ("Negative").
   * The "Decision": It happened the moment you tilted the table, not halfway through the roll.
   * The "NFE Peak at τ ≈ 0.67": This is the point of **maximum dynamical resistance**—where the ball fights hardest to escape the gravitational pull of the default basin. It's not a "choice"; it's a geometric maneuver.

### Why "Global Bias" is Scientifically Stronger

In the "Decision Point" version, we would have been claiming that a continuous system behaves discretely—a "jump" in logic. That's mathematically awkward.

In the "Global Bias" version, our claims align perfectly with the mathematics:
* **Equation:** dH/dτ = F(H, u) (`dH/dtau = F(H, u)`)
* **Reality:** Since u is an input to F at every depth, it *must* affect the dynamics globally. The null probe result confirms the math does exactly what it says.

**Key Insight:** We have proven that **Control is Context**. The control signal doesn't overwrite a specific "sentiment neuron" at layer 3. Instead, it fundamentally alters the physics of the latent space so that "Negative" becomes a possible path from the very beginning.

---

## 6. Error Correction (The Debugging Phase)

Science is a process of error elimination. The initial failure of the steering experiment (where all probabilities were identical) provided a falsification of our training protocol.

**The Refutation:** The model failed to steer when trained end-to-end without warmup.
**The Diagnosis:** We identified that the *auxiliary hypothesis* (that "simultaneous training is sufficient") was false. The model cannot steer a concept it hasn't yet learned to recognize.

**The Correction:** We adopted a phased training approach (Warmup → Control Embed → Hybrid Unfreeze). This was not an *ad hoc* save to rescue the theory, but a procedural correction that led to reproducible, high-accuracy results (98%/88% success).

---

## 7. Summary of Validated Claims

| Claim | Status | Evidence |
|-------|--------|----------|
| ODE blocks can replace transformer layers | **Corroborated** | Stable training, better loss |
| Control signal can steer semantics | **Corroborated** | 98%/88% accuracy |
| Control operates on continuous manifold | **Corroborated** | Smooth sigmoid interpolation |
| Model learns true continuous dynamics | **Corroborated** | 0.068% solver divergence |
| Efficiency is impractical | **Falsified** | Latency parity achieved |
| Late "decision point" exists | **Falsified** | Flat probe accuracy across depths |
| Control tilts landscape globally | **Corroborated** | Consistent with null probe result |
| NFE reveals geometric structure | **Corroborated** | Two distinct regimes (20 vs 14 NFE) |

---

## 8. Future Conjectures (Next Steps)

Our theory is not "true" in an absolute sense; it is merely the best current explanation that has survived these specific tests. It remains open to falsification by:

1.  **Scaling:** Will the efficiency gains and geometric findings hold at 124M parameters?
2.  **Complex Controls:** Can it handle multi-dimensional controls (e.g., Formality + Sentiment) without chaotic interference?
3.  **Different Domains:** Does the "Dynamics of Resistance" framework generalize to other tasks beyond sentiment?

We invite future work to attempt to refute these extensions.

---

## 9. Methodological Contributions

Beyond the specific architecture, this project contributes two methodological tools:

### 9.1 The Solver Invariance Test

A standardized diagnostic for continuous-depth architectures:

1. Train with a fixed-step solver (e.g., 4 Euler steps)
2. At inference, compare trajectory to a high-precision adaptive solver (e.g., Dopri5)
3. Report relative divergence

**Interpretation:**
* < 1% divergence: Model has learned intrinsic continuous dynamics
* > 5% divergence: Model may have overfitted to discretization artifacts

### 9.2 NFE as Geometry Probe

Using adaptive solver step counts to reveal geometric structure:

1. Enable NFE counting during adaptive solving
2. Sweep control signal across semantic range
3. Plot NFE vs control value

**Interpretation:**
* Sudden NFE changes indicate dynamical regime boundaries
* Consistently high NFE indicates geometrically complex regions
* NFE asymmetries reveal interactions between control and intrinsic biases
